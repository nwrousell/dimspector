---
source: tests/snapshots.rs
expression: output
---
def relative_attention_cross(x_q: [batch, seq_q, d_model], x_kv: [batch, seq_k, d_model], W_q: [d_model, heads, d_k], W_k: [d_model, heads, d_k], W_v: [d_model, heads, d_v], rel_pos_bias: [seq_k, seq_q]):
  bb0:
    ''
    query = torch.transpose(torch.reshape(x_q @ torch.reshape(W_q, (x_q.shape[2], -1)), (x_q.shape[0], x_q.shape[1], W_q.shape[1], W_q.shape[2])), 1, 2)
    key = torch.transpose(torch.reshape(x_kv @ torch.reshape(W_k, (x_kv.shape[2], -1)), (x_kv.shape[0], x_kv.shape[1], W_k.shape[1], W_k.shape[2])), 1, 2)
    value = torch.transpose(torch.reshape(x_kv @ torch.reshape(W_v, (x_kv.shape[2], -1)), (x_kv.shape[0], x_kv.shape[1], W_v.shape[1], W_v.shape[2])), 1, 2)
    scores = query @ torch.transpose(key, -1, -2)
    biased = scores + rel_pos_bias
    weights = torch.softmax(biased, dim=-1)
    out = weights @ value
    return out
