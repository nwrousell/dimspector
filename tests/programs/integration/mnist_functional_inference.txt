def linear(x: [batch, in], weight: [out, in], bias: [out]) -> [batch, out]:
  bb0:
    'Apply a linear transformation: x @ weight.T + bias.'
    return x @ torch.transpose(weight) + bias


def init_weight_matrix(in_features: in, out_features: out) -> [out, in]:
  bb0:
    'Initialize a weight matrix with Kaiming initialization.'
    weight: {⟙} = torch.randn(out_features, in_features) * 2 / in_features ** 0.5
    weight.requires_grad_(true)
    return weight


def forward(x: [batch, 1, 28, 28], w1: [hidden, 784], b1: [hidden], w2: [hidden, hidden], b2: [hidden], w3: [classes, hidden], b3: [classes]) -> [batch, classes]:
  bb0:
    'Forward pass through the MLP.'
    x: {[batch, 784]} = torch.view(x, (x.shape[0], -1))
    x: {[batch, hidden]} = torch.nn.functional.relu(linear(x, w1, b1))
    x: {[batch, hidden]} = torch.nn.functional.relu(linear(x, w2, b2))
    x: {[batch, classes]} = linear(x, w3, b3)
    return x


def main():
  bb0:
    batch_size: {64} = 64
    learning_rate: {⟙} = 0.001
    epochs: {10} = 10
    input_size: {784} = 784
    hidden_size: {256} = 256
    num_classes: {10} = 10
    num_train: {1000} = 1000
    num_test: {200} = 200
    device: {⟙} = torch.device('cuda')
    print(None)
    w1: {[256, 784]} = init_weight_matrix(input_size, hidden_size)
    b1: {[256]} = init_bias_vector(hidden_size)
    w2: {[256, 256]} = init_weight_matrix(hidden_size, hidden_size)
    b2: {[256]} = init_bias_vector(hidden_size)
    w3: {[10, 256]} = init_weight_matrix(hidden_size, num_classes)
    b3: {[10]} = init_bias_vector(num_classes)
    w1: {[256, 784]} = w1.to(device)
    b1: {[256]} = b1.to(device)
    w2: {[256, 256]} = w2.to(device)
    b2: {[256]} = b2.to(device)
    w3: {[10, 256]} = w3.to(device)
    b3: {[10]} = b3.to(device)
    train_images: {[1000, 1, 28, 28]} = torch.zeros(num_train, 1, 28, 28)
    train_labels: {[1000]} = torch.randint(0, num_classes, (num_train))
    test_images: {[200, 1, 28, 28]} = torch.zeros(num_test, 1, 28, 28)
    test_labels: {[200]} = torch.randint(0, num_classes, (num_test))
    train_images: {[1000, 1, 28, 28]} = train_images.to(device)
    train_labels: {[1000]} = train_labels.to(device)
    test_images: {[200, 1, 28, 28]} = test_images.to(device)
    test_labels: {[200]} = test_labels.to(device)
    optimizer: {⟙} = torch.optim.Adam((w1, b1, w2, b2, w3, b3), lr=learning_rate)
    num_train_batches: {16} = num_train + batch_size - 1 // batch_size
    num_test_batches: {4} = num_test + batch_size - 1 // batch_size
    jump bb1
  bb1:
    jmp bb2 if ? else bb3
  bb3:
    print('-' * 60)
    print(None)
    return
  bb2:
    total_train_loss: {⟙} = 0
    jump bb4
  bb4:
    jmp bb5 if ? else bb6
  bb6:
    train_loss: {⟙} = total_train_loss / num_train_batches
    total_test_loss: {⟙} = 0
    correct: {0} = 0
    total: {0} = 0
    torch.no_grad()
    jump bb7
  bb7:
    jmp bb8 if ? else bb9
  bb9:
    test_loss: {⟙} = total_test_loss / num_test_batches
    test_acc: {⟙} = correct / total
    print(None)
    jump bb1
  bb8:
    start: {⟙} = i * batch_size
    end: {⟙} = min(start + batch_size, num_test)
    images: {⟙} = test_images[start:end]
    labels: {⟙} = test_labels[start:end]
    outputs: {⟙} = forward(images, w1, b1, w2, b2, w3, b3)
    loss: {⟙} = torch.nn.functional.cross_entropy(outputs, labels)
    total_test_loss: {⟙} = total_test_loss + loss.item()
    predicted: {⟙} = outputs.max(1)[1]
    correct: {⟙} = correct + predicted.eq.sum.item()
    total: {} = total + labels.shape[0]
    jump bb7
  bb5:
    start: {⟙} = i * batch_size
    end: {⟙} = min(start + batch_size, num_train)
    images: {⟙} = train_images[start:end]
    labels: {⟙} = train_labels[start:end]
    optimizer.zero_grad()
    outputs: {⟙} = forward(images, w1, b1, w2, b2, w3, b3)
    loss: {⟙} = torch.nn.functional.cross_entropy(outputs, labels)
    loss.backward()
    optimizer.step()
    total_train_loss: {⟙} = total_train_loss + loss.item()
    jump bb4


def init_bias_vector(out_features: out) -> [out]:
  bb0:
    'Initialize a bias vector with zeros.'
    bias: {[out]} = torch.zeros(out_features)
    bias.requires_grad_(true)
    return bias


